# Zillow-Home-Value-Prediction-Pipeline
This project builds an end-to-end machine learning pipeline to predict Zillow home value log error using structured property and transaction data. It includes data cleaning, feature engineering, model training, evaluation, visualization, and model persistence.
Features

Combines 2016â€“2017 Zillow datasets (train and property data)

Handles missing values and encodes categorical features

Generates advanced engineered features (age, ratios, etc.)

Trains and evaluates multiple models:

Ridge Regression

Lasso Regression

ElasticNet

Random Forest Regressor

XGBoost

LightGBM

Automatically selects the best model (highest RÂ²)

Saves plots and model files to the specified local directory
zelo/
â”‚
â”œâ”€â”€ train_2016_v2.csv
â”œâ”€â”€ train_2017.csv
â”œâ”€â”€ properties_2016.csv
â”œâ”€â”€ properties_2017.csv
â”‚
â”œâ”€â”€ model_performance_comparison.png
â”œâ”€â”€ feature_importances_lightgbm.png
â”œâ”€â”€ actual_vs_predicted_logerror.png
â”œâ”€â”€ best_model_<ModelName>.pkl
â””â”€â”€ Zillow_Final_Pipeline.py
Model Evaluation Metrics

Each model is evaluated using:

RÂ² Score (Goodness of fit)

MAE (Mean Absolute Error)

RMSE (Root Mean Square Error)

Results are displayed and saved as visual comparisons in bar charts.

ğŸ“Š Visual Outputs

model_performance_comparison.png â€“ compares RÂ² scores of all models

feature_importances_lightgbm.png â€“ shows top predictive features

actual_vs_predicted_logerror.png â€“ scatter plot of actual vs predicted log errors

ğŸ’¾ Output Files
All models and plots are saved automatically in your local directory:
C:\Users\kulsum ansari\OneDrive\Documents\datathon\zelo
Tech Stack

Python (Pandas, NumPy, Scikit-learn, Seaborn, Matplotlib)

LightGBM, XGBoost

Joblib (model persistence)
Execution Steps

Place all required CSV files in the zelo folder.

Run the Python file:
python Zillow.ipynb
Check the console for progress and evaluation metrics.

View saved plots and .pkl model in the zelo folder.
ğŸ“¥ Loading datasets...
âœ… Merged dataset shape: (167888, 61)
âš™ï¸ Feature Engineering...
Train: (131623, 63), Test: (32906, 63), Total features: 63
âœ… Ridge | RÂ²=0.0079 | MAE=0.0531 | RMSE=0.0842 | Time=0.2s
âœ… Lasso | RÂ²=0.0047 | MAE=0.0531 | RMSE=0.0843 | Time=1.5s
âœ… ElasticNet | RÂ²=0.0070 | MAE=0.0531 | RMSE=0.0842 | Time=3.3s
âœ… RandomForest | RÂ²=0.0193 | MAE=0.0528 | RMSE=0.0837 | Time=565.8s
âœ… XGBoost | RÂ²=0.0260 | MAE=0.0527 | RMSE=0.0834 | Time=18.0s

ğŸš€ Training LightGBM (custom tuned)...
Training until validation scores don't improve for 100 rounds
[100]	train's rmse: 0.0824859	valid's rmse: 0.0835268
[200]	train's rmse: 0.0812908	valid's rmse: 0.0833366
[300]	train's rmse: 0.080348	valid's rmse: 0.0832885
[400]	train's rmse: 0.0795165	valid's rmse: 0.0832635
[500]	train's rmse: 0.0787489	valid's rmse: 0.0832542
Early stopping, best iteration is:
[480]	train's rmse: 0.0789102	valid's rmse: 0.0832448
âœ… LightGBM | RÂ²=0.0296 | MAE=0.0526 | RMSE=0.0832

ğŸ“Š Model Comparison:
           Model       MAE      RMSE        RÂ²
5      LightGBM  0.052570  0.083245  0.029627
4       XGBoost  0.052681  0.083398  0.026049
3  RandomForest  0.052781  0.083688  0.019262
0         Ridge  0.053093  0.084172  0.007887
2    ElasticNet  0.053066  0.084210  0.006999
1         Lasso  0.053123  0.084309  0.004664
âœ… Model comparison plot saved in zelo folder.
âœ… Feature importance plot saved in zelo folder.
âœ… Actual vs Predicted scatter plot saved in zelo folder.

ğŸ† Best Model: LightGBM | RÂ²=0.0296
ğŸ’¾ Model saved successfully at: C:\Users\kulsum ansari\OneDrive\Documents\datathon\zelo\best_model_LightGBM.pkl

